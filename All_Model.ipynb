{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c6950f9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.io import arff\n",
    "import nbimporter\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from scipy.stats import norm\n",
    "import random\n",
    "# random.seed(42)\n",
    "from Logistic_Regression import LogisticRegression\n",
    "from Naive_Bayes import NaiveBayes\n",
    "from prepare_for_training import prepare_for_training\n",
    "from sigmoid import sigmoid\n",
    "from operator import itemgetter\n",
    "import copy\n",
    "import time\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a123b3fd",
   "metadata": {},
   "source": [
    "# 讀取.arff資料集"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "082db42c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_arff(file):\n",
    "    data,features = arff.loadarff(file)\n",
    "    df = pd.DataFrame(data)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56650482",
   "metadata": {},
   "source": [
    "# 讀取.data的資料集"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d819ba3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_data(file,sep_type = \",\",header = 0):\n",
    "    df = pd.read_csv(file,sep = sep_type,header = header)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9d78328",
   "metadata": {},
   "source": [
    "# 資料切割"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a3ce0d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def data_split(X,y,k):\n",
    "    #切割後的結果\n",
    "    result = []\n",
    "    #資料筆數\n",
    "    num_examples = X.shape[0]\n",
    "    #特徵數量\n",
    "    num_features = X.shape[1]\n",
    "    #fold大小\n",
    "    fold_size = num_examples//k;\n",
    "    \n",
    "    for i in range(k):\n",
    "        start = i * fold_size;\n",
    "        end = (i + 1) * fold_size\n",
    "        \n",
    "        #訓練集\n",
    "        x_train = pd.concat([X.iloc[:start],X.iloc[end:]],axis = 0).values\n",
    "        y_train = pd.concat([y.iloc[:start],y.iloc[end:]],axis = 0).values\n",
    "\n",
    "        #測試集\n",
    "        x_test = X.iloc[start:end].values\n",
    "        y_test = y.iloc[start:end].values\n",
    "        \n",
    "        result.append([x_train,y_train,x_test,y_test])\n",
    "    \n",
    "    return result\n",
    "    \n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96566213",
   "metadata": {},
   "source": [
    "# 羅吉斯迴歸"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3c170489",
   "metadata": {},
   "outputs": [],
   "source": [
    "def LR(X,y,k,num_iterations,alpha):\n",
    "    num_examples = X.shape[0]\n",
    "    fold_size = num_examples//k;\n",
    "    accuracies = []\n",
    "    \n",
    "    for i in range(k):\n",
    "        start = i * fold_size;\n",
    "        end = (i + 1) * fold_size\n",
    "       \n",
    "        #測試集\n",
    "        x_test = X.iloc[start:end].values\n",
    "        y_test = y.iloc[start:end].values\n",
    "        \n",
    "        #將測試集做資料前處理\n",
    "        (x_test, features_mean, features_deviation) = prepare_for_training(x_test, True)\n",
    "            \n",
    "        #訓練集\n",
    "        x_train = pd.concat([X.iloc[:start],X.iloc[end:]],axis = 0).values\n",
    "        y_train = pd.concat([y.iloc[:start],y.iloc[end:]],axis = 0).values\n",
    "        \n",
    "        #訓練集資料權重\n",
    "        data_weight = np.ones((x_train.shape[0],1))\n",
    "        \n",
    "        #訓練羅吉斯迴歸模型\n",
    "        logistic_regression = LogisticRegression(x_train,y_train,data_weight)\n",
    "        (theta,cost_history) = logistic_regression.train(alpha,num_iterations)\n",
    "        \n",
    "#         print(theta.T)\n",
    "        print(\"-\"*79)\n",
    "        \n",
    "        #將測試集做預測\n",
    "        prediction = LogisticRegression.hypothesis(x_test,theta)\n",
    "        prediction[prediction > 0.5] = 1\n",
    "        prediction[prediction < 0.5] = 0\n",
    "        #計算模型正確率\n",
    "        acc = np.mean(prediction == y_test)\n",
    "        #紀錄k次的模型正確率\n",
    "        accuracies.append(acc)\n",
    "    \n",
    "    avg_accuracy = sum(accuracies) / len(accuracies)    \n",
    "    return accuracies,avg_accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87439d08",
   "metadata": {},
   "source": [
    "# 袋裝法-羅吉斯迴歸"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9a581ff1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def bagging(X,y,k,num_iterations,alpha,size):\n",
    "    num_examples = X.shape[0]\n",
    "    num_features = X.shape[1]\n",
    "    fold_size = num_examples//k;\n",
    "    accuracies = []\n",
    "    bagging_prediction = np.zeros((fold_size,1))\n",
    "    #紀錄5倍交叉驗證的所有模型\n",
    "    base_model_record = []\n",
    "    #紀錄5倍交叉驗證所有base model的預測結果\n",
    "    base_model_prediction_record = []\n",
    "    \n",
    "    for i in range(k):\n",
    "#         print(\"=\"*79)\n",
    "#         print(f\"第{i+1}次的交叉驗證\")\n",
    "#         print(\"=\"*79)\n",
    "        start = i * fold_size;\n",
    "        end = (i + 1) * fold_size\n",
    "\n",
    "        #測試集\n",
    "        x_test = X.iloc[start:end].values\n",
    "        y_test = y.iloc[start:end].values\n",
    "        \n",
    "        #訓練集\n",
    "        x_train = pd.concat([X.iloc[:start],X.iloc[end:]],axis = 0).values\n",
    "        y_train = pd.concat([y.iloc[:start],y.iloc[end:]],axis = 0).values\n",
    "        \n",
    "\n",
    "        #將測試集做資料前處理\n",
    "        (x_test, features_mean, features_deviation) = prepare_for_training(x_test, True)\n",
    "\n",
    "        \n",
    "        #子訓練集的大小，假設100%\n",
    "        sample_size = int(1*x_train.shape[0])\n",
    "        \n",
    "        #子訓練集資料權重\n",
    "        data_weight = np.ones((sample_size,1))\n",
    "        \n",
    "        #紀錄當前交叉驗證的25個基本模型\n",
    "        temp_base_model_record = []\n",
    "        \n",
    "        #訓練基本模型\n",
    "        for j in range(size):\n",
    "            #隨機抽取100%的子訓練集做模型的訓練\n",
    "            random_indices = np.random.choice(x_train.shape[0],size = sample_size, replace = True)\n",
    "            x_train_subset = x_train[random_indices,:]\n",
    "            y_train_subset = y_train[random_indices,:]\n",
    "            \n",
    "\n",
    "            #訓練羅吉斯迴歸基本模型\n",
    "            logistic_regression = LogisticRegression(x_train_subset,y_train_subset,data_weight)\n",
    "            (theta,cost_history) = logistic_regression.train(alpha,num_iterations)\n",
    "            \n",
    "            #紀錄基本模型\n",
    "            temp_base_model_record.append(theta)\n",
    "            \n",
    "        prediction = np.zeros((fold_size,1))\n",
    "            \n",
    "        #將該集成模型做預測\n",
    "        final_ensemble_base_learner_prediction = np.zeros((fold_size,size))\n",
    "        final_erlr_prediction = np.zeros((fold_size,1))\n",
    "\n",
    "        \n",
    "        for current_index in range(size):\n",
    "            final_ensemble_base_learner_x_test_prediction = sigmoid(np.dot(x_test,temp_base_model_record[current_index]))\n",
    "            final_ensemble_base_learner_x_test_prediction[final_ensemble_base_learner_x_test_prediction > 0.5] = 1\n",
    "            final_ensemble_base_learner_x_test_prediction[final_ensemble_base_learner_x_test_prediction < 0.5] = 0\n",
    "\n",
    "            final_ensemble_base_learner_prediction[:,current_index] = final_ensemble_base_learner_x_test_prediction.ravel()\n",
    "        \n",
    "        base_model_prediction_record.append(final_ensemble_base_learner_prediction)\n",
    "        \n",
    "\n",
    "        #進行集成投票\n",
    "        for row in range(final_ensemble_base_learner_prediction.shape[0]):\n",
    "            zero_nums = 0\n",
    "            one_nums = 0\n",
    "            for ele in final_ensemble_base_learner_prediction[row]:\n",
    "                if ele == 0:\n",
    "                    zero_nums+=1\n",
    "                else:\n",
    "                    one_nums+=1\n",
    "            if(zero_nums > one_nums):\n",
    "                final_erlr_prediction[row] = 0\n",
    "            else:\n",
    "                final_erlr_prediction[row] = 1\n",
    "\n",
    "        #計算模型正確率\n",
    "        acc = np.mean(final_erlr_prediction == y_test)\n",
    "        #紀錄k次的模型正確率\n",
    "        accuracies.append(acc)\n",
    "        #紀錄此次交叉驗證的25個基本模型的紀錄\n",
    "        base_model_record.append(temp_base_model_record)\n",
    "        \n",
    "    avg_accuracy = sum(accuracies) / len(accuracies)\n",
    "    return accuracies,avg_accuracy,base_model_record,base_model_prediction_record"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9916d82a",
   "metadata": {},
   "source": [
    "# ERLR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "df6f6507",
   "metadata": {},
   "outputs": [],
   "source": [
    "def erlr_base_model(X,features_deviation):      \n",
    "    #隨機產生羅吉斯迴歸基本模型之參數\n",
    "    #首先隨機產生0到1的亂數\n",
    "    theta = np.random.rand(X.shape[1], 1)\n",
    "        \n",
    "    #計算資料集的標準差\n",
    "    features_deviation = features_deviation.reshape(len(features_deviation),1)\n",
    "    features_deviation = np.insert(features_deviation, 0, np.ones(1), axis=0)\n",
    "\n",
    "    #代入反向標準常態函數，得到一般迴歸係數\n",
    "    theta = norm.ppf(theta)    \n",
    "    \n",
    "    return theta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "eaee64ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "#初始分類門檻值\n",
    "def initial_threshold(X,y,size = 5000):\n",
    "    X = X.values\n",
    "    y = y.values\n",
    "    accuracies = []\n",
    "    model_record = []\n",
    "    \n",
    "    #對資料集做資料前處理\n",
    "    (X,features_mean,features_deviation) = prepare_for_training(X)\n",
    "    \n",
    "    temp_accuracies = []\n",
    "    for i in range(size):\n",
    "        theta = erlr_base_model(X,features_deviation)\n",
    "        model = np.copy(theta).ravel().tolist()\n",
    "        model_record.append(model)\n",
    "\n",
    "        predictions = sigmoid(np.dot(X,theta))\n",
    "        predictions[predictions > 0.5] = 1\n",
    "        predictions[predictions < 0.5] = 0\n",
    "\n",
    "        acc = np.mean(predictions == y)\n",
    "        temp_accuracies.append(acc)\n",
    "        \n",
    "    return max(temp_accuracies)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "2b231244",
   "metadata": {},
   "outputs": [],
   "source": [
    "def erlr(X,y,k,size,num_iterations):\n",
    "    #資料筆數\n",
    "    num_examples = X.shape[0]\n",
    "    #特徵數量\n",
    "    num_features = X.shape[1]\n",
    "    #fold大小\n",
    "    fold_size = num_examples//k;\n",
    "    #紀錄r次迭代的集成模型分類正確率\n",
    "    r_accuracies_record = []\n",
    "    #紀錄5-fold的分類正確率\n",
    "    accuracies = []\n",
    "    #總共產生多少基本模型\n",
    "    total_base_learner = 0\n",
    "    \n",
    "     \n",
    "    for i in range(k):\n",
    "        start = i * fold_size;\n",
    "        end = (i + 1) * fold_size\n",
    "\n",
    "        #測試集\n",
    "        x_test = X.iloc[start:end].values\n",
    "        y_test = y.iloc[start:end].values\n",
    "\n",
    "        #將測試集做資料前處理\n",
    "        (x_test, features_mean, features_deviation) = prepare_for_training(x_test, True)\n",
    "\n",
    "        #訓練集\n",
    "        x_train = pd.concat([X.iloc[:start],X.iloc[end:]],axis = 0).values\n",
    "        y_train = pd.concat([y.iloc[:start],y.iloc[end:]],axis = 0).values\n",
    "        \n",
    "        #對訓練集做資料前處理\n",
    "        (x_train,features_mean,features_deviation) = prepare_for_training(x_train,True)\n",
    "        \n",
    "        #分類正確率門檻值\n",
    "        threshold = initial_threshold(X,y)\n",
    "        \n",
    "        #r次迭代的分類正確率\n",
    "        r_accuracies = []\n",
    "        \n",
    "        #r次迭代的模型\n",
    "        r_model_record = []\n",
    "        \n",
    "        #迭代逐步提升分類正確率門檻值\n",
    "        for iteration in range(num_iterations):\n",
    "            #紀錄模型的係數\n",
    "            temp_r_model = []\n",
    "            \n",
    "            #紀錄訓練集分類正確率\n",
    "            x_train_accuracies = []\n",
    "            \n",
    "            #目前基本模型的數量\n",
    "            base_learner_size = 0\n",
    "            \n",
    "            #當前基本模型的預測結果\n",
    "            current_base_learners_prediction = np.zeros((x_train.shape[0], size))\n",
    "            \n",
    "            #當前erlr集成模型預測\n",
    "            current_erlr_prediction = np.zeros((x_train.shape[0],1))\n",
    "            \n",
    "            print(\"當前分類正確率門檻值:\",threshold)\n",
    "            \n",
    "            #訓練基本模型\n",
    "            while base_learner_size < size: \n",
    "                #隨機產生羅吉斯迴歸模型 \n",
    "                theta = erlr_base_model(x_train,features_deviation)\n",
    "\n",
    "                #增加一個模型\n",
    "                total_base_learner += 1\n",
    "\n",
    "                #將訓練集做預測\n",
    "                x_train_predictions = sigmoid(np.dot(x_train,theta))\n",
    "                x_train_predictions[x_train_predictions > 0.5] = 1\n",
    "                x_train_predictions[x_train_predictions < 0.5] = 0\n",
    "                x_train_acc = np.mean(x_train_predictions == y_train)\n",
    "\n",
    "                #如果基本模型在訓練集上的分類正確率優於threshold\n",
    "                if(x_train_acc > threshold):\n",
    "                    #紀錄基本模型的係數\n",
    "                    temp_r_model.append(theta)\n",
    "                    #將該模型加入至集成模型\n",
    "                    current_base_learners_prediction[:,base_learner_size] = x_train_predictions.ravel()\n",
    "                    base_learner_size += 1\n",
    "                    \n",
    "                    print(iteration,base_learner_size)\n",
    "#                     print(theta.T)\n",
    "                    print(\"-\"*70)\n",
    "                    \n",
    "                    x_train_accuracies.append(x_train_acc)\n",
    "\n",
    "            \n",
    "            #紀錄當前集成模型\n",
    "            r_model_record.append(temp_r_model)\n",
    "            \n",
    "            #進行當前集成模型於訓練集上的投票\n",
    "            for row in range(current_base_learners_prediction.shape[0]):\n",
    "                zero_nums = 0\n",
    "                one_nums = 0\n",
    "                for ele in current_base_learners_prediction[row]:\n",
    "                    if ele == 0:\n",
    "                        zero_nums+=1\n",
    "                    else:\n",
    "                        one_nums+=1\n",
    "                if(zero_nums > one_nums):\n",
    "                    current_erlr_prediction[row] = 0\n",
    "                else:\n",
    "                    current_erlr_prediction[row] = 1\n",
    "\n",
    "            #計算模型正確率\n",
    "            acc = np.mean(current_erlr_prediction == y_train)\n",
    "            print(f\"第{iteration}次更新門檻值，當前門檻值:{threshold} ,當前集成模型在訓練集上的正確率:{acc}\")\n",
    "            r_accuracies.append(acc)\n",
    "            \n",
    "            #更新分類正確率門檻值\n",
    "            threshold = sum(x_train_accuracies)/len(x_train_accuracies)\n",
    "\n",
    "        #挑出五次門檻值更新中，集成模型正確率最高的索引\n",
    "        best_ensemble_model_acc = max(r_accuracies)\n",
    "        max_index = r_accuracies.index(best_ensemble_model_acc)\n",
    "        \n",
    "        #將該集成模型做預測\n",
    "        final_ensemble_base_learner_prediction = np.zeros((fold_size,size))\n",
    "        final_erlr_prediction = np.zeros((fold_size,1))\n",
    "\n",
    "        \n",
    "        for current_index in range(size):\n",
    "            final_ensemble_base_learner_x_test_prediction = sigmoid(np.dot(x_test,r_model_record[max_index][current_index]))\n",
    "            final_ensemble_base_learner_x_test_prediction[final_ensemble_base_learner_x_test_prediction > 0.5] = 1\n",
    "            final_ensemble_base_learner_x_test_prediction[final_ensemble_base_learner_x_test_prediction < 0.5] = 0\n",
    "\n",
    "            final_ensemble_base_learner_prediction[:,current_index] = final_ensemble_base_learner_x_test_prediction.ravel()\n",
    "        \n",
    "        \n",
    "        \n",
    "        #進行最終集成模型投票，其預測結果即為當前fold的集成模型分類正確率\n",
    "        for row in range(final_ensemble_base_learner_prediction.shape[0]):\n",
    "            zero_nums = 0\n",
    "            one_nums = 0\n",
    "            for ele in final_ensemble_base_learner_prediction[row]:\n",
    "                if ele == 0:\n",
    "                    zero_nums+=1\n",
    "                else:\n",
    "                    one_nums+=1\n",
    "            if(zero_nums > one_nums):\n",
    "                final_erlr_prediction[row] = 0\n",
    "            else:\n",
    "                final_erlr_prediction[row] = 1\n",
    "                \n",
    "        #計算最終集成模型在測試集上的正確率\n",
    "        final_ensemble_acc = np.mean(final_erlr_prediction == y_test)\n",
    "#         print(\"=\"*79)\n",
    "        print(f\"當前交叉驗證:{i} ,最終集成模型在測試集上的正確率:{final_ensemble_acc}\")\n",
    "#         print(\"=\"*79)\n",
    "        \n",
    "        #紀錄k次的模型正確率\n",
    "        accuracies.append(final_ensemble_acc)\n",
    "    avg_accuracy = sum(accuracies) / len(accuracies)\n",
    "    return accuracies,avg_accuracy,r_accuracies_record"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f8a7b76",
   "metadata": {},
   "source": [
    "# 粒子群優化演算法 - 羅吉斯迴歸"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "2a118fab",
   "metadata": {},
   "outputs": [],
   "source": [
    "def particle_swarm_optimization(X,y,n,threshold,best_model = None):\n",
    "    #設定粒子的搜索範圍\n",
    "    x_min = -2\n",
    "    x_max = 2\n",
    "    #設定粒子的速度\n",
    "    v_min = -2\n",
    "    v_max = 2\n",
    "    #權重\n",
    "    w = 0.9\n",
    "    #自我學習因子\n",
    "    c1 = 2\n",
    "    #群體學習因子\n",
    "    c2 = 2\n",
    "    #維度\n",
    "    d = X.shape[1]\n",
    "    #迭代次數\n",
    "    generation = 100\n",
    "    #紀錄每一個粒子的位置向量(生成N個粒子)\n",
    "    particle_x_record = np.random.uniform(x_min,x_max, size=(n,d))\n",
    "    \n",
    "    #是否保留上一輪的最佳粒子\n",
    "    if best_model is not None:\n",
    "#         print(\"加入上一輪的最佳粒子\")\n",
    "        particle_x_record[0] = best_model\n",
    "    \n",
    "    #紀錄每一個粒子的速度向量\n",
    "    particle_v_record = np.random.uniform(x_min,x_max, size=(n,d))\n",
    "    #紀錄每一個粒子的歷史最佳位置\n",
    "    pbest = np.zeros((n,d))\n",
    "    #紀錄每一個粒子的歷史最佳適應值\n",
    "    pbest_fitness = np.zeros(n)\n",
    "    #紀錄群體的最佳位置\n",
    "    gbest = np.zeros((n,d))\n",
    "    #紀錄群體最佳的歷史最佳適應值\n",
    "    gbest_fitness = 0\n",
    "\n",
    "    \n",
    "    #迭代generation次\n",
    "    for gen in range(generation):\n",
    "#         print(\"當前回合為:\", gen+1)\n",
    "        #評估各粒子的適應值\n",
    "        for i in range(n):\n",
    "            x_train_predictions = sigmoid(np.dot(X,particle_x_record[i].reshape(d,1)))\n",
    "            x_train_predictions[x_train_predictions > 0.5] = 1\n",
    "            x_train_predictions[x_train_predictions < 0.5] = 0\n",
    "            x_train_acc = np.mean(x_train_predictions == y) \n",
    "            #如果當前粒子的適應值大於粒子歷史最佳適應值\n",
    "            if x_train_acc > pbest_fitness[i]:\n",
    "                #更新粒子個體歷史最佳適應值\n",
    "                pbest_fitness[i] = x_train_acc\n",
    "                #更新粒子個體歷史最佳位置\n",
    "                pbest[i] = particle_x_record[i]\n",
    "        \n",
    "        #取得當前粒子群體最佳適應值的索引\n",
    "        max_index = np.argmax(pbest_fitness)\n",
    "        \n",
    "        #如果當前粒子群體最佳適應值大於歷史粒子群體最佳適應值\n",
    "        if pbest_fitness[max_index] > gbest_fitness:\n",
    "            #更新粒子群體歷史最佳位置\n",
    "            for i in range(n):\n",
    "                gbest[i] = pbest[max_index]\n",
    "            #更新粒子群體歷史最佳適應值\n",
    "            gbest_fitness = pbest_fitness[max_index]\n",
    "#         print(\"最佳適應值:\",gbest_fitness)\n",
    "        \n",
    "        #檢查粒子群體是否收斂\n",
    "        is_converge = True\n",
    "        first_particle = pbest[0]\n",
    "        for particle in pbest:\n",
    "            if not np.array_equal(pbest[0], particle):\n",
    "                is_converge = False\n",
    "                break\n",
    "\n",
    "        #如果收斂則跳出演算法\n",
    "        if is_converge:\n",
    "#             print(\"收斂了\")\n",
    "            break\n",
    "    \n",
    "        #更新粒子當前速度\n",
    "        r1 = np.random.random()\n",
    "        r2 = np.random.random()\n",
    "        particle_v_record = w * particle_v_record + c1*r1*(pbest-particle_x_record) + c2*r2*(gbest-particle_x_record)\n",
    "        w = 0.5 / generation-1\n",
    "        \n",
    "        #速度限制檢查\n",
    "        for row in range(len(particle_v_record)):\n",
    "            for col in range(len(particle_v_record[row])):\n",
    "                if particle_v_record[row][col] < v_min or particle_v_record[row][col] > v_max:\n",
    "                    particle_v_record[row][col] = np.random.uniform(v_min,v_max)\n",
    "        \n",
    "        #更新粒子的位置\n",
    "        particle_x_record = particle_x_record + particle_v_record\n",
    "        \n",
    "        #位置限制檢查\n",
    "        for row in range(len(particle_x_record)):\n",
    "            for col in range(len(particle_x_record[row])):\n",
    "                if particle_x_record[row][col] < x_min or particle_x_record[row][col] > x_max:\n",
    "                    particle_x_record[row][col] = np.random.uniform(x_min, x_max)\n",
    "                    \n",
    "    return gbest[0],gbest_fitness"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39ec520b",
   "metadata": {},
   "source": [
    "# PSO-ERLR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "e1d36ebc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def pso_erlr(X, y, k = 5, size = 25, num_iterations = 5):\n",
    "    #資料筆數\n",
    "    num_examples = X.shape[0]\n",
    "    #特徵數量\n",
    "    num_features = X.shape[1]\n",
    "    #fold大小\n",
    "    fold_size = num_examples//k;\n",
    "    #紀錄r次迭代的集成模型分類正確率\n",
    "    r_accuracies_record = []\n",
    "    #紀錄5-fold的分類正確率\n",
    "    accuracies = []\n",
    "    #總共產生多少基本模型\n",
    "    total_base_learner = 0\n",
    "    #紀錄五次交叉驗證後每個最佳集成模型的基本模型\n",
    "    base_model_record = []\n",
    "    #紀錄5倍交叉驗證後每個最佳集成模型中所有基本模型的預測結果\n",
    "    base_model_prediction_record = []\n",
    "\n",
    "    \n",
    "    #五倍交叉驗證  \n",
    "    for i in range(k):\n",
    "#         print(f\"交叉驗證{i}\")\n",
    "        start = i * fold_size;\n",
    "        end = (i + 1) * fold_size\n",
    "\n",
    "        #測試集\n",
    "        x_test = X.iloc[start:end].values\n",
    "        y_test = y.iloc[start:end].values\n",
    "        \n",
    "\n",
    "        #訓練集\n",
    "        x_train = pd.concat([X.iloc[:start],X.iloc[end:]],axis = 0).values\n",
    "        y_train = pd.concat([y.iloc[:start],y.iloc[end:]],axis = 0).values\n",
    "        \n",
    "        \n",
    "        #將測試集做資料前處理\n",
    "        (x_test, features_mean, features_deviation) = prepare_for_training(x_test, True)\n",
    "                \n",
    "        #對訓練集做資料前處理\n",
    "        (x_train,features_mean,features_deviation) = prepare_for_training(x_train,True)\n",
    "        \n",
    "        #分類正確率門檻值\n",
    "        threshold = 0\n",
    "        \n",
    "        #r次迭代的分類正確率\n",
    "        r_accuracies = []\n",
    "        \n",
    "        #r次迭代的模型\n",
    "        r_model_record = []\n",
    "        \n",
    "        #迭代逐步提升分類正確率門檻值\n",
    "        for iteration in range(num_iterations):\n",
    "#             print(f\"第{iteration}次更新分類正確率門檻值\")\n",
    "            \n",
    "            #紀錄模型的係數\n",
    "            temp_r_model = []\n",
    "            \n",
    "            #紀錄訓練集分類正確率\n",
    "            x_train_accuracies = []\n",
    "            \n",
    "            #目前基本模型的數量\n",
    "            base_learner_size = 0\n",
    "            \n",
    "            #當前基本模型的預測結果\n",
    "            current_base_learners_prediction = np.zeros((x_train.shape[0], size))\n",
    "            \n",
    "            #當前pso-erlr集成模型預測\n",
    "            current_pso_erlr_prediction = np.zeros((x_train.shape[0],1))\n",
    "            \n",
    "#             print(\"當前分類正確率門檻值:\",threshold)\n",
    "            #訓練基本模型\n",
    "            while base_learner_size < size: \n",
    "                best_model = None\n",
    "                #是否要離開尋找base learner的過程\n",
    "                exit_find_model = False\n",
    "                #紀錄重新執行粒子群優化演算法的次數(如果執行5次都沒辦法通過分類正確率門檻值就不要了)\n",
    "                count_repeat_num = 0\n",
    "                \n",
    "                #執行粒子群優化演算法\n",
    "                while count_repeat_num < 5:\n",
    "                    #挑選最佳粒子(模型)\n",
    "                    best_model_theta, best_modle_acc = particle_swarm_optimization(x_train,y_train,50,threshold,best_model) \n",
    "                    #如果最佳模型在訓練集上的分類正確率優於threshold\n",
    "                    if(best_modle_acc > threshold):\n",
    "                        #紀錄基本模型的係數\n",
    "                        temp_r_model.append(best_model_theta)      \n",
    "                       \n",
    "                        #將訓練集集做預測\n",
    "                        base_learner_x_train_predictions = sigmoid(np.dot(x_train,best_model_theta.T))\n",
    "                        base_learner_x_train_predictions[base_learner_x_train_predictions > 0.5] = 1\n",
    "                        base_learner_x_train_predictions[base_learner_x_train_predictions < 0.5] = 0\n",
    "                        \n",
    "                        #加入至集成模型\n",
    "                        current_base_learners_prediction[:,base_learner_size] = base_learner_x_train_predictions.ravel()\n",
    "                        base_learner_size += 1\n",
    "#                         print(iteration,base_learner_size)\n",
    "#                         print(best_model_theta)\n",
    "#                         print(f\"已經找到{base_learner_size}個基本模型\")\n",
    "#                         print(\"-\"*70)\n",
    "                        x_train_accuracies.append(best_modle_acc)\n",
    "                        break\n",
    "                    else:\n",
    "#                         print(\"保留最佳粒子\")\n",
    "                        #保留最佳粒子\n",
    "                        best_model = best_model_theta\n",
    "                        count_repeat_num += 1\n",
    "                \n",
    "                if count_repeat_num == 5:\n",
    "#                     print(f\"已經嘗試過{count_repeat_num}次尋找通過門檻值的基本模型了\")\n",
    "                    exit_find_model = True\n",
    "                \n",
    "                if exit_find_model:\n",
    "                    break\n",
    "            \n",
    "            #如果沒有找到25個基本模型就跳出\n",
    "            if base_learner_size < size:\n",
    "                break\n",
    "            \n",
    "            #紀錄當前集成模型\n",
    "            r_model_record.append(temp_r_model)\n",
    "            \n",
    "            \n",
    "\n",
    "            #進行當前集成模型於訓練集上的投票\n",
    "            for row in range(current_base_learners_prediction.shape[0]):\n",
    "                zero_nums = 0\n",
    "                one_nums = 0\n",
    "                for ele in current_base_learners_prediction[row]:\n",
    "                    if ele == 0:\n",
    "                        zero_nums+=1\n",
    "                    else:\n",
    "                        one_nums+=1\n",
    "                if(zero_nums > one_nums):\n",
    "                    current_pso_erlr_prediction[row] = 0\n",
    "                else:\n",
    "                    current_pso_erlr_prediction[row] = 1\n",
    "\n",
    "            #計算集成模型正確率\n",
    "            acc = np.mean(current_pso_erlr_prediction == y_train)\n",
    "#             print(\"=\"*79)\n",
    "#             print(f\"第{iteration}次更新門檻值，當前門檻值:{threshold} ,當前集成模型在訓練集上的正確率:{acc}\")\n",
    "#             print(\"=\"*79)\n",
    "            r_accuracies.append(acc)\n",
    "            \n",
    "            #更新分類正確率門檻值\n",
    "            threshold = sum(x_train_accuracies)/len(x_train_accuracies)\n",
    "        \n",
    "        #挑選出五次門檻值更新中，集成模型正確率最高的索引\n",
    "        best_ensemble_model_acc = max(r_accuracies)\n",
    "        max_index = r_accuracies.index(best_ensemble_model_acc)\n",
    "#         print(\"=\"*79)\n",
    "#         print(f\"最佳集成模型的索引是{max_index}\")\n",
    "#         print(\"=\"*79)\n",
    "    \n",
    "#         #把每一次交叉驗證中，r次的集成模型分類正確率記起來\n",
    "#         r_accuracies_record.append(r_accuracies)\n",
    "        \n",
    "        \n",
    "        #將該集成模型做預測\n",
    "        final_ensemble_base_learner_prediction = np.zeros((fold_size,size))\n",
    "        final_pso_erlr_prediction = np.zeros((fold_size,1))\n",
    "        \n",
    "        for current_index in range(size):\n",
    "            final_ensemble_base_learner_x_test_prediction = sigmoid(np.dot(x_test,r_model_record[max_index][current_index].T))\n",
    "            final_ensemble_base_learner_x_test_prediction[final_ensemble_base_learner_x_test_prediction > 0.5] = 1\n",
    "            final_ensemble_base_learner_x_test_prediction[final_ensemble_base_learner_x_test_prediction < 0.5] = 0\n",
    "\n",
    "            final_ensemble_base_learner_prediction[:,current_index] = final_ensemble_base_learner_x_test_prediction.ravel()\n",
    "        \n",
    "        \n",
    "        base_model_prediction_record.append(final_ensemble_base_learner_prediction)\n",
    "        \n",
    "        \n",
    "        #進行最終集成模型投票，其預測結果即為當前fold的集成模型分類正確率\n",
    "        for row in range(final_ensemble_base_learner_prediction.shape[0]):\n",
    "            zero_nums = 0\n",
    "            one_nums = 0\n",
    "            for ele in final_ensemble_base_learner_prediction[row]:\n",
    "                if ele == 0:\n",
    "                    zero_nums+=1\n",
    "                else:\n",
    "                    one_nums+=1\n",
    "            if(zero_nums > one_nums):\n",
    "                final_pso_erlr_prediction[row] = 0\n",
    "            else:\n",
    "                final_pso_erlr_prediction[row] = 1\n",
    "                \n",
    "        #計算最終集成模型在測試集上的正確率\n",
    "        final_ensemble_acc = np.mean(final_pso_erlr_prediction == y_test)\n",
    "#         print(\"=\"*79)\n",
    "#         print(f\"當前交叉驗證:{i} ,最終集成模型在測試集上的正確率:{final_ensemble_acc}\")\n",
    "#         print(\"=\"*79)\n",
    "        \n",
    "        \n",
    "        #紀錄k次的模型正確率\n",
    "        accuracies.append(final_ensemble_acc)\n",
    "    avg_accuracy = sum(accuracies) / len(accuracies)\n",
    "    return accuracies,avg_accuracy,r_accuracies_record,base_model_record,base_model_prediction_record"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4454dc4",
   "metadata": {},
   "source": [
    "# Naive Bayes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "79c9762b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def NB(X,y,k,is_categorical = False):\n",
    "    num_examples = X.shape[0]\n",
    "    #紀錄資料集中，每一個屬性的所有可能值\n",
    "    feature_info = {}\n",
    "    #如果是類別型資料\n",
    "    if is_categorical:\n",
    "        #每一個特徵的可能值依據資料集而決定\n",
    "        for i in range(X.shape[1]):\n",
    "            feature_info[i] = list(X.iloc[:,i].unique())\n",
    "    #如果是連續型資料\n",
    "    else:\n",
    "        #每一個特徵的可能值會用10-equal width來分割，所以會有0~9的值\n",
    "        for i in range(X.shape[1]):\n",
    "            feature_info[i] = [i for i in range(10)]\n",
    "        \n",
    "    fold_size = num_examples//k;\n",
    "    accuracies = []\n",
    "    \n",
    "    for i in range(k):\n",
    "        start = i * fold_size;\n",
    "        end = (i + 1) * fold_size\n",
    "       \n",
    "        #測試集\n",
    "        x_test = X.iloc[start:end].values\n",
    "        y_test = y.iloc[start:end].values\n",
    "            \n",
    "        #訓練集\n",
    "        x_train = pd.concat([X.iloc[:start],X.iloc[end:]],axis = 0).values\n",
    "        y_train = pd.concat([y.iloc[:start],y.iloc[end:]],axis = 0).values\n",
    "        \n",
    "        \n",
    "        #訓練簡易貝氏模型\n",
    "        naive_bayes = NaiveBayes(x_train, y_train, feature_info)\n",
    "        class_prior, likelihood, all_class = naive_bayes.fit()\n",
    "        \n",
    "        \n",
    "        #將測試集做預測\n",
    "        prediction = NaiveBayes.predict(x_test,class_prior,likelihood,all_class)\n",
    "        \n",
    "        #計算模型正確率\n",
    "        acc = np.mean(prediction == y_test)\n",
    "        \n",
    "        #紀錄k次的模型正確率\n",
    "        accuracies.append(acc)\n",
    "        \n",
    "    \n",
    "    avg_accuracy = sum(accuracies) / len(accuracies)    \n",
    "    return accuracies,avg_accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fef0cc04",
   "metadata": {},
   "source": [
    "# 袋裝法 - 簡易貝氏"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "69b0acc2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def nb_bagging(X,y,k,size,is_categorical = False):\n",
    "    num_examples = X.shape[0]\n",
    "    fold_size = num_examples//k;\n",
    "    accuracies = []\n",
    "    nb_bagging_prediction = np.zeros((fold_size,1))\n",
    "    #紀錄5倍交叉驗證的所有模型\n",
    "    base_model_record = []\n",
    "    #紀錄5倍交叉驗證所有base model的預測結果\n",
    "    base_model_prediction_record = []\n",
    "    \n",
    "    #紀錄資料集中，每一個屬性的所有可能值\n",
    "    feature_info = {}\n",
    "    #如果是類別型資料\n",
    "    if is_categorical:\n",
    "        #每一個特徵的可能值依據資料集而決定\n",
    "        for i in range(X.shape[1]):\n",
    "            feature_info[i] = list(X.iloc[:,i].unique())\n",
    "    #如果是連續型資料\n",
    "    else:\n",
    "        #每一個特徵的可能值會用10-equal width來分割，所以會有0~9的值\n",
    "        for i in range(X.shape[1]):\n",
    "            feature_info[i] = [i for i in range(10)]\n",
    "    \n",
    "              \n",
    "    #所有類別種類\n",
    "    all_class = y.iloc[:,0].unique()\n",
    "        \n",
    "    \n",
    "    for i in range(k):\n",
    "#         print(\"=\"*79)\n",
    "#         print(f\"第{i+1}次的交叉驗證\")\n",
    "#         print(\"=\"*79)\n",
    "\n",
    "        start = i * fold_size;\n",
    "        end = (i + 1) * fold_size\n",
    "\n",
    "        #測試集\n",
    "        x_test = X.iloc[start:end].values\n",
    "        y_test = y.iloc[start:end].values\n",
    "\n",
    "        #訓練集\n",
    "        x_train = pd.concat([X.iloc[:start],X.iloc[end:]],axis = 0).values\n",
    "        y_train = pd.concat([y.iloc[:start],y.iloc[end:]],axis = 0).values\n",
    "        \n",
    "        #子訓練集的大小，假設100%\n",
    "        sample_size = int(1*x_train.shape[0])\n",
    "        \n",
    "        #子訓練集資料權重\n",
    "        data_weight = np.ones((sample_size,1))\n",
    "        \n",
    "        #紀錄當前交叉驗證的25個基本模型\n",
    "        temp_base_model_record = []\n",
    "        \n",
    "        \n",
    "        #訓練基本模型\n",
    "        for j in range(size):\n",
    "            #隨機抽取100%的子訓練集做模型的訓練\n",
    "            random_indices = np.random.choice(x_train.shape[0],size = sample_size, replace = True)\n",
    "            x_train_subset = x_train[random_indices,:]\n",
    "            y_train_subset = y_train[random_indices,:]\n",
    "            \n",
    "\n",
    "            #訓練簡易貝氏模型\n",
    "            naive_bayes = NaiveBayes(x_train_subset, y_train_subset, feature_info)\n",
    "            class_prior, likelihood, all_class = naive_bayes.fit()\n",
    "           \n",
    "            temp_base_model_record.append([class_prior,likelihood])\n",
    "        \n",
    "        \n",
    "        #將該集成模型做預測\n",
    "        final_ensemble_base_learner_prediction = np.zeros((fold_size,size))\n",
    "        final_prediction = np.zeros((fold_size,1))\n",
    "\n",
    "        \n",
    "        for current_index in range(size):\n",
    "            current_class_prior,current_likelihoods = temp_base_model_record[current_index]\n",
    "            final_ensemble_base_learner_x_test_prediction = predict(x_test,all_class,current_likelihoods,current_class_prior)\n",
    "            final_ensemble_base_learner_prediction[:,current_index] = final_ensemble_base_learner_x_test_prediction.ravel()\n",
    "        \n",
    "        \n",
    "        base_model_prediction_record.append(final_ensemble_base_learner_prediction)\n",
    "            \n",
    "#             #將測試集做預測\n",
    "#             x_test_predictions = NaiveBayes.predict(x_test,class_prior,likelihood,all_class)\n",
    "            \n",
    "#             base_learners_prediction[:,j] = x_test_predictions.ravel()\n",
    "            \n",
    "#             print(j+1)\n",
    "#             print(\"-\"*70)\n",
    "\n",
    "        \n",
    "#         base_model_prediction_record.append(base_learners_prediction)\n",
    "            \n",
    "        \n",
    "        #進行集成投票\n",
    "        for row in range(final_ensemble_base_learner_prediction.shape[0]):\n",
    "            ensemble_prediction_record = {key: 0 for key in all_class}\n",
    "            for ele in final_ensemble_base_learner_prediction[row]:\n",
    "                ensemble_prediction_record[int(ele)] += 1\n",
    "            ensemble_prediction = max(ensemble_prediction_record, key=lambda k: ensemble_prediction_record[k])                \n",
    "            final_prediction[row] = ensemble_prediction\n",
    "\n",
    "        #計算模型正確率\n",
    "        acc = np.mean(final_prediction == y_test)\n",
    "        #紀錄k次的模型正確率\n",
    "        accuracies.append(acc)\n",
    "        #紀錄此次交叉驗證的25個基本模型的紀錄\n",
    "        base_model_record.append(temp_base_model_record)\n",
    "        \n",
    "    avg_accuracy = sum(accuracies) / len(accuracies)\n",
    "    return accuracies,avg_accuracy,base_model_record,base_model_prediction_record"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f2f7d20",
   "metadata": {},
   "source": [
    "# TRENB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "c5549f84",
   "metadata": {},
   "outputs": [],
   "source": [
    "#隨機產生prior probability #add的目的用來產生粒子群的速度\n",
    "def random_prior_probability(all_class,add = False):\n",
    "    class_prior = {}\n",
    "    if add:\n",
    "        for unique_class in all_class:\n",
    "            class_prior[unique_class] = (np.random.rand() + 1e-10)*1e-1\n",
    "    else:\n",
    "        for unique_class in all_class:\n",
    "            class_prior[unique_class] = np.random.rand() + 1e-10\n",
    "            \n",
    "    return class_prior"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "a640d5f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "#隨機產生likelihood #add的目的用來產生粒子群的速度\n",
    "def random_likelihood(num_features,feature_info,all_class,add = False):\n",
    "    likelihoods = {}\n",
    "    \n",
    "    if add:\n",
    "        #針對每一個特徵\n",
    "        for feature in range(num_features):\n",
    "            #針對每個特徵值\n",
    "            for i in feature_info[feature]:\n",
    "                likelihoods[str(feature)+\"_\"+str(i)] = {}\n",
    "                #針對每一個類別\n",
    "                for unique_class in all_class:\n",
    "                    likelihoods[str(feature)+\"_\"+str(i)][unique_class] = (np.random.rand() + 1e-10)*1e-1\n",
    "    else:\n",
    "        #針對每一個特徵\n",
    "        for feature in range(num_features):\n",
    "            #針對每個特徵值\n",
    "            for i in feature_info[feature]:\n",
    "                likelihoods[str(feature)+\"_\"+str(i)] = {}\n",
    "                #針對每一個類別\n",
    "                for unique_class in all_class:\n",
    "                    likelihoods[str(feature)+\"_\"+str(i)][unique_class] = np.random.rand() + 1e-10\n",
    "    return likelihoods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "915e7eab",
   "metadata": {},
   "outputs": [],
   "source": [
    "#將prior probability做正規化\n",
    "def prior_probability_normalization(class_prior):\n",
    "    total_prior_probability = sum(class_prior.values())\n",
    "    for key, value in class_prior.items():\n",
    "        class_prior[key] = value / total_prior_probability"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "d197806b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#將likelihood做正規化\n",
    "def likelihood_normalization(likelihoods,num_features,feature_info,all_class):\n",
    "    total_likelihood = {} \n",
    "    for feature in range(num_features):\n",
    "        total_likelihood[feature] = {}\n",
    "        #針對每一個類別\n",
    "        for unique_class in all_class:\n",
    "            total = 0      \n",
    "            #針對每個特徵值\n",
    "            for i in feature_info[feature]: \n",
    "                total += likelihoods[str(feature)+\"_\"+str(i)][unique_class]\n",
    "            \n",
    "            total_likelihood[feature][unique_class] = total\n",
    "            \n",
    "    for feature in range(num_features):\n",
    "        #針對每個特徵值\n",
    "        for i in feature_info[feature]:\n",
    "            #針對每一個類別\n",
    "            for unique_class in all_class:\n",
    "                likelihoods[str(feature)+\"_\"+str(i)][unique_class] /= total_likelihood[feature][unique_class]\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "b84e5259",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 隨機產生簡易貝氏分類模型\n",
    "def trenb_base_model(X,feature_info,all_class):\n",
    "    #特徵數量\n",
    "    num_features = X.shape[1]   \n",
    "    \n",
    "    \n",
    "    #隨機產生prior probability\n",
    "    class_prior = random_prior_probability(all_class)\n",
    "    #隨機產生likelihood\n",
    "    likelihoods = random_likelihood(num_features,feature_info,all_class)\n",
    "    \n",
    "    \n",
    "    #將prior probability做正規化\n",
    "    prior_probability_normalization(class_prior)\n",
    "    #將likelihood做正規化\n",
    "    likelihood_normalization(likelihoods,num_features,feature_info,all_class)\n",
    "            \n",
    "    return class_prior,likelihoods      "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "dbde57e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "#(隨機)簡易貝氏進行預測\n",
    "def predict(X,all_class,likelihoods,class_prior):\n",
    "    predictions = np.zeros((X.shape[0], 1))\n",
    "    #進行預測\n",
    "    #每筆資料\n",
    "    for row in range(X.shape[0]):\n",
    "        max_value = max(all_class)\n",
    "        single_class_prediction = {}\n",
    "        #每個類別\n",
    "        for unique_class in all_class:\n",
    "            cur_likelihood = 1\n",
    "            #每個特徵\n",
    "            for col in range(X.shape[1]):\n",
    "                cur_likelihood *= likelihoods[str(col)+\"_\"+str(X[row][col])][unique_class]\n",
    "            single_class_prediction[unique_class] = cur_likelihood * class_prior[unique_class]\n",
    "\n",
    "        #機率值最大的類別即為人預測結果\n",
    "        predictions[row][0] = max(single_class_prediction, key=lambda k: single_class_prediction[k])\n",
    "    \n",
    "    return predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "0dbb85ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 初始分類門檻值\n",
    "def trenb_initial_threshold(X,y,feature_info,all_class,num_iter = 20,size = 1000): \n",
    "    X = X.values\n",
    "    y = y.values\n",
    "    accuracies = []\n",
    "    model_record = []\n",
    "    predictions = np.zeros((X.shape[0], 1))\n",
    "    \n",
    "    \n",
    "    for iteration in range(num_iter):\n",
    "        temp_accuracies = []\n",
    "        for i in range(size):\n",
    "            #隨機產生簡易貝氏分類模型\n",
    "            class_prior,likelihoods = trenb_base_model(X,feature_info,all_class)\n",
    "            \n",
    "            #進行預測\n",
    "            predictions = predict(X,all_class,likelihoods,class_prior)\n",
    "            \n",
    "            acc = np.mean(predictions == y)\n",
    "            temp_accuracies.append(acc)\n",
    "       \n",
    "        accuracies.append(max(temp_accuracies))\n",
    "    \n",
    "    initial_threshold = sum(accuracies) / len(accuracies)\n",
    "    return initial_threshold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "4cf05135",
   "metadata": {},
   "outputs": [],
   "source": [
    "def trenb(X,y,k,size,num_iterations,is_categorical = False):\n",
    "    #資料筆數\n",
    "    num_examples = X.shape[0]\n",
    "    #特徵數量\n",
    "    num_features = X.shape[1]\n",
    "    #fold大小\n",
    "    fold_size = num_examples//k;\n",
    "    #紀錄r次迭代的集成模型分類正確率\n",
    "    r_accuracies_record = []\n",
    "    #紀錄5-fold的分類正確率\n",
    "    accuracies = []\n",
    "    #總共產生多少基本模型\n",
    "    total_base_learner = 0\n",
    "    #紀錄資料集中，每一個屬性的所有可能值\n",
    "    feature_info = {}\n",
    "    #如果是類別型資料\n",
    "    if is_categorical:\n",
    "        #每一個特徵的可能值依據資料集而決定\n",
    "        for i in range(X.shape[1]):\n",
    "            feature_info[i] = list(X.iloc[:,i].unique())\n",
    "    #如果是連續型資料\n",
    "    else:\n",
    "        #每一個特徵的可能值會用10-equal width來分割，所以會有0~9的值\n",
    "        for i in range(X.shape[1]):\n",
    "            feature_info[i] = [i for i in range(10)]\n",
    "              \n",
    "    #所有類別種類\n",
    "    all_class = np.unique(y)\n",
    "    \n",
    "     \n",
    "    for i in range(k):\n",
    "        print(f\"交叉驗證{i}\")\n",
    "        start = i * fold_size;\n",
    "        end = (i + 1) * fold_size\n",
    "\n",
    "        #測試集\n",
    "        x_test = X.iloc[start:end].values\n",
    "        y_test = y.iloc[start:end].values\n",
    "\n",
    "        #訓練集\n",
    "        x_train = pd.concat([X.iloc[:start],X.iloc[end:]],axis = 0).values\n",
    "        y_train = pd.concat([y.iloc[:start],y.iloc[end:]],axis = 0).values\n",
    "        \n",
    "        #分類正確率門檻值\n",
    "        threshold = trenb_initial_threshold(X,y,feature_info,all_class,num_iter = 20,size = 1000)\n",
    "        \n",
    "        #r次迭代的分類正確率\n",
    "        r_accuracies = []\n",
    "        \n",
    "        #r次迭代的模型\n",
    "        r_model_record = []\n",
    "        \n",
    "        #迭代逐步提升分類正確率門檻值\n",
    "        for iteration in range(num_iterations):\n",
    "            #紀錄模型\n",
    "            temp_r_model = []\n",
    "            \n",
    "            #紀錄訓練集分類正確率\n",
    "            x_train_accuracies = []\n",
    "            \n",
    "            #目前基本模型的數量\n",
    "            base_learner_size = 0\n",
    "            \n",
    "            #當前基本模型的預測結果\n",
    "            current_base_learners_prediction = np.zeros((x_train.shape[0], size))\n",
    "            \n",
    "            #當前trenb集成模型預測\n",
    "            current_trenb_prediction = np.zeros((x_train.shape[0],1))\n",
    "            \n",
    "            print(\"當前分類正確率門檻值:\",threshold)\n",
    "            #訓練基本模型\n",
    "            while base_learner_size < size: \n",
    "                #隨機產生簡易貝氏分類器\n",
    "                class_prior,likelihoods = trenb_base_model(X,feature_info,all_class)      \n",
    "\n",
    "                #增加一個模型\n",
    "                total_base_learner += 1\n",
    "\n",
    "                #將訓練集做預測\n",
    "                x_train_predictions = predict(x_train,all_class,likelihoods,class_prior)\n",
    "                x_train_acc = np.mean(x_train_predictions == y_train)\n",
    "\n",
    "                #如果基本模型在訓練集上的分類正確率優於threshold\n",
    "                if(x_train_acc > threshold):\n",
    "                    #紀錄基本模型的資訊\n",
    "                    temp_r_model.append([class_prior,likelihoods])\n",
    "                    #將該模型加入至集成模型\n",
    "                    current_base_learners_prediction[:,base_learner_size] = x_train_predictions.ravel()\n",
    "                    base_learner_size += 1\n",
    "                    print(iteration,base_learner_size)\n",
    "#                     print(likelihoods)\n",
    "#                     print(class_prior)\n",
    "                    print(\"-\"*70)\n",
    "                    x_train_accuracies.append(x_train_acc)\n",
    "\n",
    "            \n",
    "            #紀錄當前集成模型\n",
    "            r_model_record.append(temp_r_model)\n",
    "\n",
    "            #進行當前集成模型於訓練集上投票\n",
    "            for row in range(current_base_learners_prediction.shape[0]):\n",
    "                ensemble_prediction_record = {key: 0 for key in all_class}\n",
    "#                 print(base_learners_prediction[row])\n",
    "                for ele in current_base_learners_prediction[row]:\n",
    "                    ensemble_prediction_record[int(ele)] += 1\n",
    "                ensemble_prediction = max(ensemble_prediction_record, key=lambda k: ensemble_prediction_record[k])                \n",
    "#                 print(f\"集成將該筆資料預測為:{ensemble_prediction}\")\n",
    "                current_trenb_prediction[row] = ensemble_prediction\n",
    "\n",
    "            #計算模型正確率\n",
    "            acc = np.mean(current_trenb_prediction == y_train)\n",
    "            print(f\"第{iteration}次更新門檻值，當前門檻值:{threshold} ,當前集成模型在訓練集上的正確率:{acc}\")\n",
    "            r_accuracies.append(acc)\n",
    "            \n",
    "            #更新分類正確率門檻值\n",
    "            threshold = sum(x_train_accuracies)/len(x_train_accuracies)\n",
    "        \n",
    "       #挑出五次門檻值更新中，集成模型正確率最高的索引\n",
    "        best_ensemble_model_acc = max(r_accuracies)\n",
    "        max_index = r_accuracies.index(best_ensemble_model_acc)\n",
    "        \n",
    "        #將該集成模型做預測\n",
    "        final_ensemble_base_learner_prediction = np.zeros((fold_size,size))\n",
    "        final_trenb_prediction = np.zeros((fold_size,1))\n",
    "\n",
    "        \n",
    "        for current_index in range(size):\n",
    "            current_class_prior,current_likelihoods = r_model_record[max_index][current_index]\n",
    "            final_ensemble_base_learner_x_test_prediction = predict(x_test,all_class,current_likelihoods,current_class_prior)\n",
    "            final_ensemble_base_learner_prediction[:,current_index] = final_ensemble_base_learner_x_test_prediction.ravel()\n",
    "        \n",
    "        #進行當前集成模型於測試集上投票\n",
    "        for row in range(final_ensemble_base_learner_prediction.shape[0]):\n",
    "            ensemble_prediction_record = {key: 0 for key in all_class}\n",
    "#                 print(base_learners_prediction[row])\n",
    "            for ele in final_ensemble_base_learner_prediction[row]:\n",
    "                ensemble_prediction_record[int(ele)] += 1\n",
    "            ensemble_prediction = max(ensemble_prediction_record, key=lambda k: ensemble_prediction_record[k])                \n",
    "#                 print(f\"集成將該筆資料預測為:{ensemble_prediction}\")\n",
    "            final_trenb_prediction[row] = ensemble_prediction \n",
    "        \n",
    "        #計算最終集成模型在測試集上的正確率\n",
    "        final_ensemble_acc = np.mean(final_trenb_prediction == y_test)\n",
    "#         print(\"=\"*79)\n",
    "        print(f\"當前交叉驗證:{i} ,最終集成模型在測試集上的正確率:{final_ensemble_acc}\")\n",
    "#         print(\"=\"*79) \n",
    "        \n",
    "        \n",
    "        #紀錄k次的模型正確率\n",
    "        accuracies.append(final_ensemble_acc)\n",
    "    avg_accuracy = sum(accuracies) / len(accuracies)\n",
    "    print(\"=\"*70)\n",
    "    return accuracies,avg_accuracy,r_accuracies_record\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71cc1e9b",
   "metadata": {},
   "source": [
    "# 粒子群優化演算法-簡易貝氏\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "087325ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "def trenb_particle_swarm_optimization(X,y,n,feature_info,all_class,threshold,best_model = None):\n",
    "    #設定粒子的搜索範圍\n",
    "    x_min = 0 + 1e-10\n",
    "    x_max = 1\n",
    "    #設定粒子的速度\n",
    "    v_min = 0 + 1e-10\n",
    "    v_max = 1\n",
    "    #權重\n",
    "    w = 0.9\n",
    "    #自我學習因子\n",
    "    c1 = 2\n",
    "    #群體學習因子\n",
    "    c2 = 2\n",
    "    #維度(特徵數量)\n",
    "    d = X.shape[1]\n",
    "    #迭代次數\n",
    "    generation = 100\n",
    "    \n",
    "    #紀錄每一個粒子的位置向量\n",
    "    particle_x_record = []\n",
    "    for i in range(n):\n",
    "        particle_x_record.append({})\n",
    "    \n",
    "    #隨機生成N個粒子\n",
    "    for i in range(n):\n",
    "        #隨機產生簡易貝氏分類器\n",
    "        class_prior,likelihoods = trenb_base_model(X,feature_info,all_class)  \n",
    "#         particle_x_record.append({\"likelihood\":likelihoods,\"class_prior\":class_prior})\n",
    "        particle_x_record[i][\"likelihood\"] = likelihoods\n",
    "        particle_x_record[i][\"class_prior\"] = class_prior \n",
    "    \n",
    "    #是否保留上一輪的最佳粒子\n",
    "    if best_model is not None:\n",
    "        print(\"加入上一輪的最佳粒子\")\n",
    "        particle_x_record[0] = best_model\n",
    "    \n",
    "    #紀錄每一個粒子的速度向量\n",
    "    particle_v_record = []\n",
    "    for i in range(n):\n",
    "        particle_v_record.append({})\n",
    "    \n",
    "    #隨機產生N個粒子的速度\n",
    "    for i in range(n):\n",
    "        #隨機產生liklihood\n",
    "        liklihoods = random_likelihood(d,feature_info,all_class,True)\n",
    "        #隨機產生prior probability\n",
    "        class_prior = random_prior_probability(all_class,True)\n",
    "        \n",
    "        particle_v_record[i][\"likelihood\"] = liklihoods\n",
    "        particle_v_record[i][\"class_prior\"] = class_prior\n",
    "    \n",
    "    \n",
    "    #紀錄每一個粒子的歷史最佳位置\n",
    "    pbest = []\n",
    "    for i in range(n):\n",
    "        pbest.append({})\n",
    "    \n",
    "    #紀錄每一個粒子的歷史最佳適應值\n",
    "    pbest_fitness = []\n",
    "    for i in range(n):\n",
    "        pbest_fitness.append(0)\n",
    "        \n",
    "    #紀錄群體的最佳位置\n",
    "    gbest = []\n",
    "    for i in range(n):\n",
    "        gbest.append({})\n",
    "        \n",
    "    #紀錄群體最佳的歷史最佳適應值\n",
    "    gbest_fitness = 0\n",
    "\n",
    "    \n",
    "    #迭代generation次\n",
    "    for gen in range(generation):\n",
    "        print(\"當前回合為:\", gen+1)\n",
    "        #評估各粒子的適應值\n",
    "        for i in range(n):      \n",
    "            likelihoods = particle_x_record[i][\"likelihood\"]\n",
    "            class_prior = particle_x_record[i][\"class_prior\"]\n",
    "            x_train_predictions = predict(X,all_class,likelihoods,class_prior)\n",
    "            x_train_acc = np.mean(x_train_predictions == y) \n",
    "#             print(f\"粒子{i}的長相\")\n",
    "#             print(particle_x_record[i])\n",
    "#             print(f\"粒子{i}的適應值:{x_train_acc}\")\n",
    "            #如果當前粒子的適應值大於粒子歷史最佳適應值\n",
    "#             print(\"=\"*79)\n",
    "#             print(\"pbest_fitness:\",pbest_fitness)\n",
    "            if x_train_acc > pbest_fitness[i]:\n",
    "                #更新粒子個體歷史最佳適應值\n",
    "                pbest_fitness[i] = x_train_acc\n",
    "                #更新粒子個體歷史最佳位置\n",
    "                pbest[i] = copy.deepcopy(particle_x_record[i])\n",
    "        \n",
    "#         print(\"=\"*79)\n",
    "#         print(\"更新完pbest\")\n",
    "#         print(pbest)\n",
    "#         print(\"=\"*79)\n",
    "        \n",
    "        #取得當前粒子群體最佳適應值的索引\n",
    "        max_index = pbest_fitness.index(max(pbest_fitness))\n",
    "#         print(f\"當前的最佳粒子為:粒子{max_index}\")\n",
    "\n",
    "        #如果當前粒子群體最佳適應值大於歷史粒子群體最佳適應值\n",
    "        if pbest_fitness[max_index] > gbest_fitness:\n",
    "            #更新粒子群體歷史最佳位置\n",
    "            for i in range(n):\n",
    "                gbest[i] = copy.deepcopy(pbest[max_index])\n",
    "            #更新粒子群體歷史最佳適應值\n",
    "            gbest_fitness = pbest_fitness[max_index]\n",
    "        \n",
    "#         print(\"更新完gbest\")\n",
    "#         print(gbest)\n",
    "#         print(\"=\"*79)\n",
    "#         print(\"gbest_fitness\")\n",
    "#         print(gbest_fitness)\n",
    "            \n",
    "        print(\"最佳適應值:\",gbest_fitness)\n",
    "#         print(\"=\"*79)\n",
    "        \n",
    "        #檢查粒子群體是否收斂\n",
    "        is_converge = True\n",
    "        for particle in pbest:\n",
    "            if particle != pbest[0]:\n",
    "                is_converge = False\n",
    "                break\n",
    "    \n",
    "        #如果收斂則跳出演算法\n",
    "        if is_converge:\n",
    "            print(\"收斂了\")\n",
    "            break\n",
    "\n",
    "        #更新粒子當前速度\n",
    "        r1 = np.random.random()\n",
    "        r2 = np.random.random()\n",
    "        \n",
    "        #針對每一個粒子\n",
    "        for i in range(n):\n",
    "            #針對每一個類別\n",
    "            for unique_class in all_class:\n",
    "                #處理prior probability\n",
    "                particle_v_record[i][\"class_prior\"][unique_class] = (w * particle_v_record[i][\"class_prior\"][unique_class] + \n",
    "                                                                     c1*r1*(pbest[i][\"class_prior\"][unique_class] - particle_x_record[i][\"class_prior\"][unique_class]) +\n",
    "                                                                     c2*r2*(gbest[i][\"class_prior\"][unique_class] - particle_x_record[i][\"class_prior\"][unique_class])\n",
    "                                                                     )\n",
    "                #針對每一個特徵\n",
    "                for feature in range(d):\n",
    "                    #針對每個特徵值\n",
    "                    for feature_value in feature_info[feature]:\n",
    "                        #處理likelihood\n",
    "                        particle_v_record[i][\"likelihood\"][str(feature)+\"_\"+str(feature_value)][unique_class] = (w * particle_v_record[i][\"likelihood\"][str(feature)+\"_\"+str(feature_value)][unique_class] + \n",
    "                                                                                                     c1*r1*(pbest[i][\"likelihood\"][str(feature)+\"_\"+str(feature_value)][unique_class] - particle_x_record[i][\"likelihood\"][str(feature)+\"_\"+str(feature_value)][unique_class]) +\n",
    "                                                                                                     c2*r2*(gbest[i][\"likelihood\"][str(feature)+\"_\"+str(feature_value)][unique_class] - particle_x_record[i][\"likelihood\"][str(feature)+\"_\"+str(feature_value)][unique_class])\n",
    "                                                                                                     )\n",
    "\n",
    "        w = 0.5 / generation-1\n",
    "        \n",
    "        \n",
    "        #速度限制檢查\n",
    "        for i in range(n):\n",
    "            #針對每一個類別\n",
    "            for unique_class in all_class:\n",
    "                if particle_v_record[i][\"class_prior\"][unique_class] < v_min or particle_v_record[i][\"class_prior\"][unique_class] > v_max:\n",
    "                    particle_v_record[i][\"class_prior\"][unique_class] = (np.random.rand() + 1e-10)*1e-1\n",
    "                \n",
    "                #針對每一個特徵\n",
    "                for feature in range(d):\n",
    "                    #針對每個特徵值\n",
    "                    for feature_value in feature_info[feature]:\n",
    "                        if particle_v_record[i][\"likelihood\"][str(feature)+\"_\"+str(feature_value)][unique_class] < v_min or particle_v_record[i][\"likelihood\"][str(feature)+\"_\"+str(feature_value)][unique_class] > v_max:\n",
    "                            particle_v_record[i][\"likelihood\"][str(feature)+\"_\"+str(feature_value)][unique_class] = (np.random.rand() + 1e-10)*1e-1\n",
    "                                          \n",
    "     \n",
    "        #更新粒子的位置\n",
    "        for i in range(n):\n",
    "            #針對每一個類別\n",
    "            for unique_class in all_class:\n",
    "                particle_x_record[i][\"class_prior\"][unique_class] += particle_v_record[i][\"class_prior\"][unique_class]\n",
    "                #針對每一個特徵\n",
    "                for feature in range(d):\n",
    "                    #針對每個特徵值\n",
    "                    for feature_value in feature_info[feature]:\n",
    "                        particle_x_record[i][\"likelihood\"][str(feature)+\"_\"+str(feature_value)][unique_class] += particle_v_record[i][\"likelihood\"][str(feature)+\"_\"+str(feature_value)][unique_class]\n",
    "                        \n",
    "#         #將各個粒子做正規化\n",
    "#         for i in range(n):\n",
    "#             class_prior = particle_x_record[i][\"class_prior\"]\n",
    "#             likelihoods = particle_x_record[i][\"likelihood\"]\n",
    "            \n",
    "#             prior_probability_normalization(class_prior)\n",
    "#             likelihood_normalization(likelihoods,d,feature_info,all_class)\n",
    "            \n",
    "#             particle_x_record[i][\"class_prior\"] = class_prior\n",
    "#             particle_x_record[i][\"likelihood\"] = likelihoods\n",
    "        \n",
    "        #位置限制檢查\n",
    "        for i in range(n):\n",
    "            #針對每一個類別\n",
    "            for unique_class in all_class:\n",
    "                if particle_x_record[i][\"class_prior\"][unique_class] < x_min or particle_x_record[i][\"class_prior\"][unique_class] > x_max:\n",
    "                    particle_x_record[i][\"class_prior\"][unique_class] = np.random.rand() + 1e-10\n",
    "                \n",
    "                #針對每一個特徵\n",
    "                for feature in range(d):\n",
    "                    #針對每個特徵值\n",
    "                    for feature_value in feature_info[feature]:\n",
    "                        if particle_x_record[i][\"likelihood\"][str(feature)+\"_\"+str(feature_value)][unique_class] < x_min or particle_x_record[i][\"likelihood\"][str(feature)+\"_\"+str(feature_value)][unique_class] > x_max:\n",
    "                            particle_x_record[i][\"likelihood\"][str(feature)+\"_\"+str(feature_value)][unique_class] = np.random.rand() + 1e-10\n",
    "                            \n",
    "        #將各個粒子做正規化\n",
    "        for i in range(n):\n",
    "            class_prior = particle_x_record[i][\"class_prior\"]\n",
    "            likelihoods = particle_x_record[i][\"likelihood\"]\n",
    "            \n",
    "            prior_probability_normalization(class_prior)\n",
    "            likelihood_normalization(likelihoods,d,feature_info,all_class)\n",
    "            \n",
    "            particle_x_record[i][\"class_prior\"] = class_prior\n",
    "            particle_x_record[i][\"likelihood\"] = likelihoods\n",
    "        \n",
    "#     print(\"現在來看看這個最佳粒子的長相\")\n",
    "#     print(gbest[0])\n",
    "#     likelihoods = gbest[0][\"likelihood\"]\n",
    "#     class_prior = gbest[0][\"class_prior\"]\n",
    "#     x_train_predictions = predict(X,all_class,likelihoods,class_prior)\n",
    "#     x_train_acc = np.mean(x_train_predictions == y)\n",
    "#     print(\"這個最佳粒子的適應值:\",x_train_acc)\n",
    "                        \n",
    "    return gbest[0],gbest_fitness"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a357e42b",
   "metadata": {},
   "source": [
    "# PSO-TRENB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "76386cbf",
   "metadata": {},
   "outputs": [],
   "source": [
    "def pso_trenb(X, y, k = 5, size = 25, num_iterations = 5, is_categorical = False):\n",
    "    #資料筆數\n",
    "    num_examples = X.shape[0]\n",
    "    #特徵數量\n",
    "    num_features = X.shape[1]\n",
    "    #fold大小\n",
    "    fold_size = num_examples//k;\n",
    "    \n",
    "    #紀錄資料集中，每一個屬性的所有可能值\n",
    "    feature_info = {}\n",
    "    #如果是類別型資料\n",
    "    if is_categorical:\n",
    "        #每一個特徵的可能值依據資料集而決定\n",
    "        for i in range(X.shape[1]):\n",
    "            feature_info[i] = list(X.iloc[:,i].unique())\n",
    "    #如果是連續型資料\n",
    "    else:\n",
    "        #每一個特徵的可能值會用10-equal width來分割，所以會有0~9的值\n",
    "        for i in range(X.shape[1]):\n",
    "            feature_info[i] = [i for i in range(10)]\n",
    "    \n",
    "#     print(feature_info)\n",
    "    \n",
    "              \n",
    "    #所有類別種類\n",
    "    all_class = y.iloc[:,0].unique()\n",
    "        \n",
    "    #紀錄r次迭代的集成模型分類正確率\n",
    "    r_accuracies_record = []\n",
    "    #紀錄5-fold的分類正確率\n",
    "    accuracies = []\n",
    "    #總共產生多少基本模型\n",
    "    total_base_learner = 0\n",
    "    #紀錄五次交叉驗證後每個最佳集成模型的基本模型\n",
    "    base_model_record = []\n",
    "    #紀錄5倍交叉驗證後每個最佳集成模型中所有基本模型的預測結果\n",
    "    base_model_prediction_record = []\n",
    "\n",
    "    \n",
    "    #五倍交叉驗證  \n",
    "    for i in range(k):\n",
    "        start_time = time.time()\n",
    "        print(f\"交叉驗證{i}\")\n",
    "        start = i * fold_size;\n",
    "        end = (i + 1) * fold_size\n",
    "\n",
    "        #測試集\n",
    "        x_test = X.iloc[start:end].values\n",
    "        y_test = y.iloc[start:end].values\n",
    "\n",
    "        #訓練集\n",
    "        x_train = pd.concat([X.iloc[:start],X.iloc[end:]],axis = 0).values\n",
    "        y_train = pd.concat([y.iloc[:start],y.iloc[end:]],axis = 0).values\n",
    "        \n",
    "        #分類正確率門檻值\n",
    "        threshold = 0\n",
    "        \n",
    "        #r次迭代的分類正確率\n",
    "        r_accuracies = []\n",
    "        \n",
    "        #r次迭代的模型\n",
    "        r_model_record = []\n",
    "        \n",
    "        #迭代逐步提升分類正確率門檻值\n",
    "        for iteration in range(num_iterations):\n",
    "            print(f\"第{iteration}次更新分類正確率門檻值\")\n",
    "            \n",
    "            #紀錄模型的係數\n",
    "            temp_r_model = []\n",
    "            \n",
    "            #紀錄訓練集分類正確率\n",
    "            x_train_accuracies = []\n",
    "            \n",
    "            #目前基本模型的數量\n",
    "            base_learner_size = 0\n",
    "            \n",
    "            #當前基本模型的預測結果\n",
    "            current_base_learners_prediction = np.zeros((x_train.shape[0], size))\n",
    "            \n",
    "            #當前pso-trenb集成模型預測\n",
    "            current_pso_trenb_prediction = np.zeros((x_train.shape[0],1))\n",
    "            \n",
    "            print(\"當前分類正確率門檻值:\",threshold)\n",
    "            \n",
    "            #訓練基本模型\n",
    "            while base_learner_size < size: \n",
    "                #當前是否保留最佳粒子\n",
    "                best_model = None\n",
    "                #是否要離開尋找base learner的過程\n",
    "                exit_find_model = False\n",
    "                #紀錄重新執行粒子群優化演算法的次數(如果執行5次都沒辦法通過分類正確率門檻值就不要了)\n",
    "                count_repeat_num = 0\n",
    "                \n",
    "                #執行粒子群優化演算法\n",
    "                while count_repeat_num < 5:\n",
    "                    #挑選最佳粒子(模型)\n",
    "                    best_nb_model, best_modle_acc = trenb_particle_swarm_optimization(x_train,y_train,50,feature_info,all_class,threshold,best_model) \n",
    "\n",
    "                    #如果最佳模型在訓練集上的分類正確率優於threshold\n",
    "                    if(best_modle_acc > threshold):\n",
    "                        #紀錄基本模型\n",
    "                        temp_r_model.append(best_nb_model)  \n",
    "                        \n",
    "                        #取得模型\n",
    "                        class_prior = best_nb_model[\"class_prior\"]\n",
    "                        likelihoods = best_nb_model[\"likelihood\"]\n",
    "                                    \n",
    "                        #將訓練集做預測\n",
    "                        x_train_predictions = predict(x_train,all_class,likelihoods,class_prior)\n",
    "                        \n",
    "                        #加入至集成模型\n",
    "                        current_base_learners_prediction[:,base_learner_size] = x_train_predictions.ravel()\n",
    "                        base_learner_size += 1\n",
    "                        \n",
    "                        print(iteration,base_learner_size)\n",
    "                        print(f\"已經找到{base_learner_size}個基本模型\")\n",
    "#                         print(best_nb_model)\n",
    "                        print(\"-\"*70)\n",
    "                        x_train_accuracies.append(best_modle_acc)\n",
    "                        break\n",
    "                    else:\n",
    "                        print(\"保留最佳粒子\")\n",
    "                        #保留最佳粒子\n",
    "                        best_model = best_nb_model\n",
    "#                         print(best_model)\n",
    "                        count_repeat_num += 1\n",
    "                \n",
    "                if count_repeat_num == 5:\n",
    "                    print(f\"已經嘗試過{count_repeat_num}次尋找通過門檻值的基本模型了\")\n",
    "                    exit_find_model = True\n",
    "                \n",
    "                if exit_find_model:\n",
    "                    break\n",
    "        \n",
    "            #如果沒有找到25個基本模型就跳出\n",
    "            if base_learner_size < size:\n",
    "                break\n",
    "            \n",
    "            \n",
    "            #紀錄當前集成模型\n",
    "            r_model_record.append(temp_r_model)\n",
    "            \n",
    "\n",
    "            #進行當前集成模型於訓練集上投票\n",
    "            for row in range(current_base_learners_prediction.shape[0]):\n",
    "                ensemble_prediction_record = {key: 0 for key in all_class}\n",
    "                for ele in current_base_learners_prediction[row]:\n",
    "                    ensemble_prediction_record[int(ele)] += 1\n",
    "                ensemble_prediction = max(ensemble_prediction_record, key=lambda k: ensemble_prediction_record[k])                \n",
    "                current_pso_trenb_prediction[row] = ensemble_prediction\n",
    "            \n",
    "            #計算集成模型正確率\n",
    "            acc = np.mean(current_pso_trenb_prediction == y_train)\n",
    "            print(f\"第{iteration}次更新門檻值，當前門檻值:{threshold} ,當前集成模型在訓練集上的正確率:{acc}\")\n",
    "            r_accuracies.append(acc)\n",
    "            \n",
    "            #更新分類正確率門檻值\n",
    "            threshold = sum(x_train_accuracies)/len(x_train_accuracies)\n",
    "    \n",
    "        \n",
    "        #挑出五次門檻值更新中，集成模型正確率最高的索引\n",
    "        best_ensemble_model_acc = max(r_accuracies)\n",
    "        max_index = r_accuracies.index(best_ensemble_model_acc)\n",
    "        \n",
    "        #將該集成模型做預測\n",
    "        final_pso_trenb_base_learner_prediction = np.zeros((fold_size,size))\n",
    "        final_pso_trenb_prediction = np.zeros((fold_size,1))\n",
    "\n",
    "        \n",
    "        for current_index in range(size):\n",
    "            current_class_prior = r_model_record[max_index][current_index][\"class_prior\"]\n",
    "            current_likelihoods = r_model_record[max_index][current_index][\"likelihood\"]\n",
    "            final_ensemble_base_learner_x_test_prediction = predict(x_test,all_class,current_likelihoods,current_class_prior)\n",
    "            final_pso_trenb_base_learner_prediction[:,current_index] = final_ensemble_base_learner_x_test_prediction.ravel()\n",
    "        \n",
    "        \n",
    "        base_model_prediction_record.append(final_pso_trenb_base_learner_prediction)\n",
    "        \n",
    "        \n",
    "        #進行當前集成模型於測試集上投票\n",
    "        for row in range(final_pso_trenb_base_learner_prediction.shape[0]):\n",
    "            ensemble_prediction_record = {key: 0 for key in all_class}\n",
    "#                 print(base_learners_prediction[row])\n",
    "            for ele in final_pso_trenb_base_learner_prediction[row]:\n",
    "                ensemble_prediction_record[int(ele)] += 1\n",
    "            ensemble_prediction = max(ensemble_prediction_record, key=lambda k: ensemble_prediction_record[k])                \n",
    "#                 print(f\"集成將該筆資料預測為:{ensemble_prediction}\")\n",
    "            final_pso_trenb_prediction[row] = ensemble_prediction \n",
    "        \n",
    "        #計算最終集成模型在測試集上的正確率\n",
    "        final_ensemble_acc = np.mean(final_pso_trenb_prediction == y_test)\n",
    "#         print(\"=\"*79)\n",
    "        print(f\"當前交叉驗證:{i} ,最終集成模型在測試集上的正確率:{final_ensemble_acc}\")\n",
    "#         print(\"=\"*79) \n",
    "        end_time = time.time()\n",
    "        execution_time = end_time - start_time\n",
    "        print(f\"第{i}次交叉驗證所花費的時間：{execution_time/60} 分鐘\")\n",
    "        \n",
    "        \n",
    "        #紀錄k次的模型正確率\n",
    "        accuracies.append(final_ensemble_acc)\n",
    "    avg_accuracy = sum(accuracies) / len(accuracies)\n",
    "    print(\"=\"*70)\n",
    "    return accuracies,avg_accuracy,r_accuracies_record,base_model_prediction_record"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3843783",
   "metadata": {},
   "source": [
    "# 取得最終結果"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "6689a70f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 取得結果\n",
    "def get_result(accuracies,avg_accuracy):\n",
    "    print(\"五次的模型正確率\")\n",
    "    print(accuracies)\n",
    "    print(\"=\"*79)\n",
    "    print(\"模型平均正確率\")\n",
    "    print(avg_accuracy)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
